---
title: "Advanced R: Intro and Foundational R"
author: "Timothée Bonnet & Robert Cope"
date: "18/08/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: lumen
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# Introduction

## What this is about

In this workshop series we will attempt to work together to improve your R-skills. We assume you already have lots of experience in R, but probably little formal computer science training. 

We do not know everything about R, and it is likely you know better than us solutions to problems you encounter regularly. This will work best if you tell us about what tasks you often need to accomplish, how you do things, propose alternatives to the code we show, and engage with exercises.

The current plan for the four sessions:

1. Today foundational R
2. 15/09 Functional programming
3. 22/09 Programming efficiency, debugging
4. 29/09 Style and standardisation, group practice (+ any leftovers/requests)

But above all the plan is to try and be flexible. Ask us if we need to cover something else, tell us if something is too simple, don't hesitate to tell how you approach a problem differently from us...


## Introduce yourself, what you would like to get of this course, in a minute.


# Foundational R

Today we will start with presenting some foundations of R coding, going further than what is typically explained in introductions to R. Maybe you know most of this already, probably you do not know it all as none of it is essential to write working code. However, knowing it will help you write more robust and efficient code. In R lots of bugs and unexpected behaviour come from ignoring foundational R structures. 

We will use a little bit of tidyverse, at least dyplr, so install/load either the full tidyverse or just dyplr.
```{r}
library(tidyverse)
```



# Data types Structures

## Elemental data types

There are 6 fundamental types of data in R:
**logical, integer, double, character**, complex and raw. I assume you don't need the last two, so let's see what they are and not mention them again: 

```{r}
is.complex(10 + 3i)
as.complex(1)
as.raw(14)
charToRaw("Bytes")
```


Now the types that are useful:

```{r}
typeof(FALSE)

typeof(c(2L, 10L, 1L))

typeof(c(2,10, 1))

typeof("A")
```

Note that `numeric` is not really a type but includes integer and double:
```{r}
is.numeric(1L)
is.numeric(1.1)

# as.numeric coerces to double, not integer
typeof(as.numeric(FALSE))
```



An atomic vector can hold only one type. If you try to combine different types they will be coerced into the most flexible one:
logical < integer < double < character.

```{r}
str(c("a", 1))
str(c(TRUE, 1L))
str(c(TRUE, 1))
```

If you want to coerce the other way around you need to be explicit about it and you may lose information. It can be useful but in general it is a bit dangerous.

```{r}
as.logical(2.3)
as.logical(0)
as.logical(0.1)
as.logical(-0.1)

as.integer(1.2)

as.double("A")
as.double("2.3")
```


## Vectors 

Vectors are the basic data structure in R and there are two types of them: atomic vectors (which we often just call "vectors") and lists. Vectors (in the broad sense) are defined by having one dimension.

Vectors have three properties:

* Type, `typeof()`, what type of fundamental data it contains.
* Length, `length()`, how many elements it has.
* Attributes, `attributes()`, optional meta-data.


Formally, lists are vectors:
```{r}
is.vector(list(c(123)))

#that is why you can create a list with:
vector(mode = "list", length = 3)
```

But unlike atomic vectors (which we often call just "vector") they can contain heterogeneous data.

If you need to know whether a data structure is an atomic vector you need to use `is.atomic()`:

```{r}
x <- list(c("bird1","bird2", "bird3"), c("skink1", "skink2", "skink3", "skink4", "skink5")) # some imput

# A function that does NOT work
countspecies <- function(x){
    if(is.vector(x)){
      size <- length(x)
    }else{
      size <- paste0(unlist(lapply(x, function(y) {length(y)})), collapse = " and ")
    }
  print(paste("There are", size, "species"))
}

countspecies(x)

#Also, attributes make it even less reliable:
x <- list(c("bird1","bird2", "bird3"), c("skink1", "skink2", "skink3", "skink4", "skink5")) # some imput
attributes(x) <- list(location = "ANU")
countspecies(x)


# A better function
countspecies2 <- function(x){
  stopifnot("In countspecies2() the argument x is not a vector or a list"=(is.atomic(x) || is.list(x) ))
    if(is.atomic(x)){
      size <- length(x)
    }else{
      size <- paste0(unlist(lapply(x, function(y) {length(y)})), collapse = " and ")
    }
  print(paste("There are", size, "species"))
}

countspecies2(x)

```



### Atomic vectors

You probably know them well, so very briefly, you create them with `c()` or `vector(mode = 'double')`

Atomic vectors are always flat (with a single dimension), even if you nest c():

```{r}
c(1, c(2, c(3, 4)))
```


### Lists

You create lists with `list()` or `vector(mode='list')`. Lists can contain any data structure and more.

```{r}
x <- list(1:3, 
          "a",
          c(TRUE, FALSE, TRUE),
          c(2.3, 5.9),
          f2 = function(a) {a^2}, 
          list("nested list"), 
          matrix(NA))
str(x)
```


Online Quiz.
Without running the code, predict what is the output of:
```{r, eval=FALSE}
c(TRUE, 1L)

length( list( list(1,2,3,4), c(5,6)))

1 == "1"

-1 < FALSE
```

### Attributes

Vectors, like any R object can have arbitrary additional attributes, used to store metadata about the object. Attributes can be thought of as a named list with unique names. Attributes can be accessed individually with `attr(object, attribute_name)` or all at once (as a list) with `attributes(object)`.

```{r}
species <- c("fish1", "grass1", "spider1")

attributes(species) <- list(date = as.Date("19/08/2021", "%d/%m/%y"), location = "ACT")

attr(species, "survey_type") <- "incidental"

attributes(species)
attr(species, which = "date")
```

Most lists and atomic vectors attributes are not preserved through subsetting or functions, so don't rely on the information being carried forward a pipeline.
```{r}
attributes(species[1])
attributes(length(species))
attributes(c(species, species))
```

The only attributes that are (sometimes) preserved through functions are:

* Names
* Dimensions
* Class

Each have dedicated functions to access or assign (you can still use attr, but is it not recommended).
```{r}
names(species) <- c("Maccullochella", "Themeda", "Atrax")
dim(species) <- c(1,3,1)
class(species) 
class(species) <- "species_array"
```

Check preserved attributes:
```{r}
names( species[1] )

# but here not:
class(species[1:2])
# or:
dim(species[1,,])
```

### Factors

If you are already familiar with factors in R you may wonder why we mention them at this stage.
Factors are built on top of **integer vectors** using two attributes: the class, “factor”, which makes them behave differently from regular integer vectors, and the levels, which defines the set of allowed values. 

```{r}
x <- factor(c("a", "b", "b", "a"))
x

attributes(x)

class(x)
levels(x)

str(x) #see the integer values
```

To be sure you understand how levels and values relate to each other let's consider and discuss the result of each line:
```{r}
(f1 <- factor(letters))
levels(f1) <- rev(levels(f1)); f1
(f2 <- rev(factor(letters)))
(f3 <- factor(letters, levels = rev(letters)))
```


### Small practice

<!-- Without running the code, try predicting the output of the 3 following expressions: -->

<!-- ```{r, eval=FALSE} -->
<!-- c(TRUE, 1L) -->
<!-- c(2.1, FALSE, 3) -->
<!-- c(NA, TRUE, "1") -->
<!-- ``` -->

<!-- What does this code return? -->
<!-- ```{r, eval=FALSE} -->
<!-- x <- c(1,3,2,4) -->
<!-- dim(x) <- c(2,2) -->
<!-- x -->
<!-- ``` -->


## Matrices and arrays

Now we know a lot about vectors you may be happy to learn that it all applies to matrices and arrays.
Matrices are vectors with two dimensions. Arrays are atomic vectors with two or more dimensions (just be mindful that matrices and arrays can look like vectors if all but one of their dimensions is of size 1... it does not change much but occasionnaly matters.)

```{r}
x <- 1:9
dim(x) # A common source of bugs!
is.matrix(x)
is.array(x)

dim(x) <- c(3,3)
is.matrix(x)
is.array(x)

dim(x) <- c(3,1,3)
is.matrix(x)
is.array(x)

x # x now has three dimensions

is.atomic(x) # but still an atomic vector!

```


## Data-frames / tibbles / data-tables

Data-frames and derived structures (tibbles, data-tables) are the most common type of data container in R. A data frame is a two dimensional list of vectors with the same size. 

You know them well already, so we are going to cover only some rare features and examples.

Since a data.frame is a list, you may think that you can nest any structure in a list element. You are right, but you need to be explicit about what you are trying to do, using the function `I()`:

Doesn't work:
```{r, error=TRUE}
data.frame(x = 1:3, y = list(1:2, 1:3, 1:4))
```

Works:
```{r}
(dt <- data.frame(x = 1:3, y = I(list(1:2, 1:3, 1:4))) ) 
dt$y
```

The same is true for tibbles and data-tables since they are an extension of data-frames:
```{r}
(dtt <- tibble(x = 1:3, y = I(list(1:2, 1:3, 1:4))) ) 

dtt$y

(dttt  <- data.table::data.table(x = 1:3, y = I(list(1:2, 1:3, 1:4))) )
dttt$y
```


## Which data structure to use when?

Complicated question... What do you think?

My personal feeling:

1. Generally data-frames are best for data. Tibbles are the same as data-frames. Data-tables are very similar but optimised for very large datasets. 
2. Matrices are best when mathematics are involved, or spatial data in raster format
3. Lists are great for intermediate objects within functions, not for user-friendly input/output
4. Vectors good for simple intermediate objects



# Subsetting

Subsetting is very common in data wrangling but can be confusing because:

* There are 3 subsetting operators: `[`, `[[``, ` $ ` (+ subsetting functions)
* 6 types of subsetting (if we work with  `x <- c(a=2.1, b=4.2,c= 3.3,d= 5.4)`:
    * Positive integers: `x[c(3, 1)]`
    * Negative integers `x[-c(3, 1)]`
    * Logical vectors `x[c(TRUE, TRUE, FALSE, FALSE)]` or `x[x > 3]`
    * Nothing `x[]`
    * Zero `x[0]` (usually not intended)
    * Character vectors for named vectors : `x[c("a", "c")]`
* Different data structures respond differently to subsetting operators; for instance:
    * `[` return a list whereas `[[` and `$` return the content of a list
    * `[[NA]]` gives an error for an atomic vector but `NULL` for a list



### Simplifying vs. preserving subsetting

Simplifying subsetting gives the simplest possible data structure that can represent the output. That is generally what you want when you script because you see more easily the output. However, preserving subsetting retains the structure of the input in the output (so if you extract one column of a data-frame you get a data-frame and not a atomic vector) which is generally safer for programming because it is more predictable.

```{r}
x <- c(a = 1, b = 2)
x[1] # preserving
x[[1]] #simplifying
```

```{r}
y <- list(a = 1, b = 2)
str(y[1]) # preserving
str(y[[1]])  #simplifying
```

```{r}
z <- factor(c("a", "b"))
z[1] # preserving
z[1, drop = TRUE] #simplifying
```

```{r}
a <- matrix(1:4, nrow = 2)
a[1, , drop = FALSE] # preserving
a[1, ] #simplifying
```

```{r}
df <- data.frame(a = 1:2, b = 1:2)
str(df[1])# preserving

str(df[[1]]) #simplifying
str(df[, "a", drop = FALSE])# preserving
str(df[, "a"]) #simplifying
```


**Practice:** This function works on the first output but not on the second, why?

```{r, eval=TRUE, error=TRUE}
dat1 <- data.frame(location = sample(letters, size=10, replace = TRUE),
                   frog.present.2018=as.logical(rbinom(100, 1, 0.1)),
                   frog.present.2019=as.logical(rbinom(100, 1, 0.2)),
                   frog.present.2020=as.logical(rbinom(100, 1, 0.25)),
                   frog.present.2021=as.logical(rbinom(100, 1, 0.4)))

dat2 <- data.frame(Location = sample(LETTERS, size=10, replace = TRUE),
                   frog.present.2021=as.logical(rbinom(100, 1, 0.4)))

f_count_frogs <- function(x){
  x <- x[, -1]
  apply(x, 2, mean)
}

f_count_frogs(dat1)
f_count_frogs(dat2)

```


By the way, if you work with tibbles you do not have to worry about that simplifying subset 
```{r, error=TRUE}
dat1 <- data.frame(location = sample(letters, size=10, replace = TRUE),
                   frog.present.2018=as.logical(rbinom(100, 1, 0.1)),
                   frog.present.2019=as.logical(rbinom(100, 1, 0.2)),
                   frog.present.2020=as.logical(rbinom(100, 1, 0.25)),
                   frog.present.2021=as.logical(rbinom(100, 1, 0.4)))

dat2 <- data.frame(Location = sample(LETTERS, size=10, replace = TRUE),
                   frog.present.2021=as.logical(rbinom(100, 1, 0.4)))

f_count_frogs <- function(x){
  x <- x[, -1]
  apply(x, 2, mean)
}

f_count_frogs(dat1)
f_count_frogs(dat2)

f_count_frogs_tidy <- function(x){
  x %>% tibble() %>% summarise(across(where(is.logical), mean)) 
}

f_count_frogs_tidy(dat1)
f_count_frogs_tidy(dat2)
```

the function `where` is similar to `which`, and `across` select is valid column one by one to apply the function mean.

### Small practice


What does this code return and why?
```{r}
data <- data.frame(location = letters[1:5], date = date(),
           species1=c(9,8,3,7,10), species2 = c(2,3,1,4,5), species3= 0)

tosubset <- factor(x = c("species3"))

data[, tosubset]

```




# Control flow

## Loops


```{r, eval=FALSE}

for (i in 1:10)
{
  print(i)
}
```

Apart from for-loops, while-loops and repeat-loops can be (rarely?) useful. Here are quick demonstrations. We will probably not mention them again unless you want us to:

```{r, eval=FALSE}
x <- 0
while(x >= -2) # be careful, this test needs to be FALSE at some point, or the loop will never stop
{
  x <- rnorm(1)
  print(x)
}
```


```{r, eval=FALSE}
repeat{
  x <- rnorm(1)
  print(x)
  if(x < -2) break # essential to stop the loop
  }
```


For-loops can be interrupted for one iteration or fully:

```{r}
for (i in 1:10) {
  if (i < 3)
    next #skip this iteration

  print(i)

  if (i >= 5)
    {
    break # end for-loop
  }
  print("end")
}
```


### Tips for better loops

1. Allocate container before loops (as much as possible)

```{r}
system.time({
a <- rnorm(10^8)
b <- vector(length = length(a))
for(i in 1:length(a))
{
  b[i] <- a[i] + 1
}
})

system.time({
a <- rnorm(10^8)
b <- vector() # Not good!
for(i in 1:length(a))
{
  b[i] <- a[i] + 1
}
}) # 3-fold increase on my computer

```



2. Use `seq_along()` instead of `1:length()`

We have some numbers stored in `a` and for each of them want to draw a set of numbers to be stored in one element of the list `b`:

```{r, eval=FALSE}
a <- rnorm(n=10)
b <- vector(mode = "list", length = length(a))
for (i in 1:length(a))
{
  b[[i]] <- runif(n=100, min = -100, max = a[i])
}
b

(a <- vector(length = 0))
b <- vector(mode = "list", length = length(a))
for (i in 1:length(a))
{
  b[[i]] <- runif(n=100, min = -100, max = a[i])
}
b # direct error but also risky for downstream code. Two things don't quite work: i==1 gives NA, i==0 gives NULL

(a <- vector(length = 0))
b <- vector(mode = "list", length = length(a))
for (i in seq_along(a))
{
  b[[i]] <- runif(n=100, min = -100, max = a[i])
}
b #cleaner, more predictable
```


3. If you find writing a loop difficult, write the code for the first element to loop over. Then copy past the code inside a loop and adjust the indices.



*Digression: Loops are often not the most efficient way to achieve a goal, especially in term of typing. They are generally disliked by a portion of R-coders. Nevertheless they can provide more flexibility than shorter alternatives, sometimes can be more explicit about what the code is really doing, and can give you a way out of difficult problems. Also, part of the loop bad reputation is based on prejudice or on the behaviour of older R-versions. In the past, data were copied within loop iterations, a process that was extremely slow, and apply-functions were then much more efficient. However, from at least R-3, provided you create a container before a loop, loops are as efficient (or more) than apply-functions. Similarly, R-4 has invalidated some old recommendations that applied for R-3; for instance, '[<-' has become much more efficient and does not need to be avoided as much within loops. Keep in mind that R is evolving, so what you know today may not be fully valid in 3 years from now.*

## Tests

The basics:

```{r, eval=FALSE}
if (condition) true_action
if (condition) true_action else false_action

ifelse(vector_of_conditions, true, false)
```


### switch

Instead of:
```{r}
x_option <- function(x) {
  if (x == "a") {
    "option 1"
  } else if (x == "b") {
    "option 2"
  } else if (x == "c") {
    "option 3"
  } else {
    stop("Invalid `x` value")
  }
}
```

Make your code more compact with `switch()`
```{r}
x_option <- function(x) {
  switch(x,
    a = "option 1",
    b = "option 2",
    c = "option 3",
    stop("Invalid `x` value")
  )
}
```

The last component of a `switch()` (and nested `if{}else{if {}else{}` } ) should always throw an error, otherwise unmatched inputs will invisibly return `NULL`

### && and || vs. & and |

Because `if` evaluates a single test, you should use && and || in if statements. Faster and safer as long as you can ensure the tests have length one (or that you are fin with only the first element being used).

```{r, error=TRUE}
x <- c(1:10)
if(is.matrix(x) && ncol(x)> 6) print("hello") else print("bye")
if(is.matrix(x) & ncol(x)> 6) print("hello") else print("bye")

```

Avoids the use of special cases tests before running the main test. Imagine that for some reason you need to compute the median of the second parameter of many data-sets and you do not much about them.

```{r, error=TRUE}
input <- list(mtcars,InsectSprays, Orange[,1, drop=F], volcano, data.frame(NULL), Titanic)

for(i in seq_along(input))
{
  print(paste("dt", i, "median=", median(input[[i]][,2])))
} #error, we need to check is.numeric!

for(i in seq_along(input))
{
  if(is.numeric(input[[i]][,2]))
  {
    print(paste("dt", i))
    print(paste("median=", median(input[[i]][,2])))
  }
} # error, we need to check dimensions!

for(i in seq_along(input))
{
  if(is.numeric(input[[i]][,2]) && ncol(input[[i]])>=2)
  {
    print(paste("dt", i))
    print(paste("median=", median(input[[i]][,2])))
  }
} # error, the first test cannot work if the second is false!

for(i in seq_along(input))
{
  if(ncol(input[[i]])>=2 && is.numeric(input[[i]][,2])) # turn around the conditions, the first test protect the second one!
  {
    print(paste("dt", i))
    print(paste("median=", median(input[[i]][,2])))
  }
} # error because last element is an array, which breaks the second test again

for(i in seq_along(input))
{
  if(is.data.frame(input[[i]]) && ncol(input[[i]])>=2 && is.numeric(input[[i]][,2]))
  {
    print(paste("dt", i))
    print(paste("median=", median(input[[i]][,2])))
  }
} # now all tests are protected.

```


```{r}
x <- 1:12
ifelse(x %% 2 == 0, "even", "odd") # divisible by 2?
ifelse(x %% 2 == 0 & x %% 3 ==0, 1, 0)# divisible by 2 and 3?
ifelse(x %% 2 == 0 | x %% 3 ==0, 1, 0)# divisible by 2 or 3?

ifelse(x %% 2 == 0 && x %% 3 ==0, 1, 0) # not correct
ifelse(x %% 2 == 0 || x %% 3 ==0, 1, 0) # not correct
```


### Small Practice

```{r}
nb_parameters <- 3
variances <- runif(n = nb_parameters, min = 0.1, max = 1)
correlations <- runif(n=nb_parameters*(nb_parameters-1)/2, min = -0.99, max=0.99)
```

Use control flow to build the matrix with the variances on the diagonal and the correlations in symmetrical off-diagonal positions (you are also allowed subset, assign and arithmetic operations). For instance for 3 parameters:

```{r, echo=FALSE}
matrix(c("A", "B", "C",
         "B",  "D", "E",
         "C", "E", "F" ), nrow = 3)
```
 where `"A", "D", "F"` are variances and the other letters are correlations.


```{r, echo=FALSE}
count <- 1
mat <- diag(nb_parameters)
for (i in 1:nb_parameters)
{
  for (j in 1:i)
    {
    if(i!=j)
      {
      mat[i,j] <- mat[j,i] <- correlations[count]
      count <- count+1
    }else{
        mat[i,j] <- variances[i] #could be before the j-loop
      }
  }
}
mat
```


<!-- # Exercise to finish -->

<!-- Based on ALA data -->

<!-- ```{r, eval=FALSE} -->
<!-- devtools::install_github("AtlasOfLivingAustralia/galah") -->
<!-- ``` -->


<!-- ```{r, eval=FALSE} -->
<!-- library(galah) -->
<!-- galah_config(atlas = "Australia", download_reason_id = 3,verbose = TRUE, email="Bonnettimothee@hotmail.fr") -->

<!-- ala_counts(taxa = select_taxa("Onychophora")) -->

<!-- ala_counts(taxa = select_taxa("Origma solitaria")) -->

<!-- # xx <- ala_species(taxa = select_taxa("Origma solitaria"), -->
<!-- #             filters = select_filters(year >= 2010, -->
<!-- #                                                 profile = "ALA")) -->

<!-- xx <- ala_occurrences(taxa = select_taxa("Onychophora"), -->
<!--             filters = select_filters(profile = "ALA")) -->
<!-- write.csv(xx, "Data/onychophora") -->

<!-- xx <- read.csv("https://timotheenivalis.github.io/data/onychophora.csv") -->

<!-- ``` -->

1. Write some code that add an attribute for extraction date and time. Try to use a format that is compatible with other date data in the extracted data, and also formally a date.
2. Write some code  that filters out records that are not at least at the species level, and print how many records were excluded.
3. For each taxon, calculate the time between the last record and the extraction date/time, and visualise those duration spatially. (Can you see trends in the spatial distribution of records?)
4. For the two taxa with most records, create a data-frame containing the number of records with dates for each taxon on each year (with years indexing rows).

```{r, echo=FALSE, eval=FALSE}
# 1.
onych <- read.csv("https://timotheenivalis.github.io/data/onychophora.csv")
attr(onych, which = "date") <- Sys.Date()
str(onych)
onych$eventDate <- as.Date(onych$eventDate)
typeof(attr(onych, which = "date"))
as.double(attr(onych, which = "date"))

# 2.
species <- unlist(lapply(strsplit(onych$scientificName, split = " "), length))>=2
onychsp <- onych[species,]
# or using stringr
str_detect(string = onych$scientificName, pattern = " ")

# 3.
onychsp$duration <- attr(onychsp, which = "date") - onychsp$eventDate

#to be completed
```

<!-- ```{r} -->
<!-- xx %>% group_by(dataResourceName) %>% filter(eventDate != "") %>% summarise(n()) -->
<!-- xx %>% group_by(dataResourceName) %>% summarise(mean(eventDate != ""), sum(eventDate != "")) -->
<!-- ``` -->


# Demonstration with base R vs. a little bit of dyplr and Pre-workshop exercises

Here is a compilation of answers, with a few explanations, for the pre-course exercise.

Note the use of The tidyverse package `dyplr`, which offers lots of efficient shortcuts in data wrangling. The syntax is completely different from most of base-R, so it is good knowing a bit how to translate between them.


1. Load the data at https://timotheenivalis.github.io/data/gapminder_wide.csv (if possible using R code)

```{r, eval=FALSE}
#1. From R studio, create a project
dir.create("RawData") # folder for external, untreated data only.
download.file(url = "https://timotheenivalis.github.io/data/gapminder_wide.csv",
              destfile = "RawData/gapminder_wide.csv")# Could download manually.
gapminder <- read_csv("RawData/gapminder_wide.csv") # can also use read.csv()
```

Or
```{r}
gapminder <- read.csv("https://timotheenivalis.github.io/data/gapminder_wide.csv")
```

The first solution is more future proof, because you do not have control on modifications or deletions of files on the Internet. The second solution is simpler if you just want to try something once and your code is not meant to be robust in the long term.

2. Transform the dataset from a wide into a long format retaining all the data but where the only columns are: `continent`, `country`, `year`, `pop`,  `lifeExp`, `gdpPercap`. That one is a bit tricky, so if you don't manage the output is at https://timotheenivalis.github.io/data/gapminder_data.csv

A base-R solution:

```{r}
yearcolumn <- function(basename, year) {
  return(paste(basename, "_", as.character(year), sep =""))
}

years <- seq(from = 1952, to = 2007, by = 5)
values <- c("pop", "lifeExp", "gdpPercap")

for (year in years) {
   yearcolumns <- yearcolumn(values, year)
   yearvalues <- gapminder[c("continent", "country", yearcolumns)]
   yearvalues["year"] = year
   colnames(yearvalues) <- c("continent", "country", values, "year")
   if (year == 1952) {
     valuesbyyear <- yearvalues
   } else {
     valuesbyyear <- rbind(valuesbyyear, yearvalues)
   }
}
#valuesbyyear is the result
```

Another base-R solution, using `do.call()`:
```{r}
col_names <- colnames(gapminder)
years <- as.numeric(
  substr(col_names[grepl("^pop_", col_names)], 5, 8)) # a bit hacky, but works

df_long <- data.frame(
  continent = rep(gapminder$continent, length(years)),
  country = rep(gapminder$country, length(years)),
  year = rep(years, each = nrow(gapminder)),
  pop = do.call(c, gapminder[, grepl("^pop_", col_names)]),
  lifeExp = do.call(c, gapminder[, grepl("^lifeExp", col_names)]),
  gdpPercap = do.call(c, gapminder[, grepl("^gdpPercap", col_names)]))
```



A tidyverse version:
```{r}
gaplong0 <- gapminder %>% pivot_longer(cols = c(starts_with("gdpPercap"), starts_with("lifeExp"), starts_with("pop")),
                           names_to = "parameter_year", values_to = "obs_val") %>%
                          separate(parameter_year, into = c('parameter', 'year'), sep = "_") %>%
                          pivot_wider(names_from = parameter, values_from = obs_val)
```

Another tidyverse version:
```{r}
test0<-pivot_longer(gapminder,
                    cols = gdpPercap_1952:pop_2007,
                    names_to = "type",
                    values_to = "measure")

test0$year<-str_sub(test0$type,-4,-1)
test0$var<-sapply(str_split(test0$type,"_"),function(x) paste(x[1]))
test0$type<-NULL
gaplong1<-pivot_wider(test0,names_from = var, values_from = measure)
```


A more efficient tidyverse solution (although it requires to know a lot about regular expressions):
```{r}
gaplong2 <- gapminder %>%
  pivot_longer(cols = -continent & -country,
               names_to = c(".value", "year"),
               names_pattern = "([^\\_]*)\\_*(\\d{4})")

identical(gaplong0, gaplong1)
identical(gaplong1, gaplong2)

#some edits to the base-R solution to check the result is identical
firstsolutioneditforcomparison <- as_tibble(valuesbyyear[order(valuesbyyear$continent, valuesbyyear$country, valuesbyyear$year),c(1,2,6,5,4,3)])
firstsolutioneditforcomparison$year <- as.character(firstsolutioneditforcomparison$year)
identical(firstsolutioneditforcomparison, gaplong2)


secondsolutioneditforcomparison <- as_tibble(df_long[order(df_long$continent, df_long$country, df_long$year),c(1,2,3,6,5,4)])
secondsolutioneditforcomparison$year <- as.character(secondsolutioneditforcomparison$year)
identical(secondsolutioneditforcomparison, gaplong2)

```


3. In a single graph plot the relationships between lifeExp and gdpPercap in each country, but only for data after the year 1990 and for Africa and Asia only.

On a log-scale and with lines:
```{r}
gaplong0 %>% filter(year >1990, continent %in% c("Africa", "Asia")) %>%
  ggplot(aes(x=gdpPercap, y=lifeExp, group=country, col=country)) + geom_line()  +
  scale_x_log10() +
  theme(legend.position = "none")
```

with non-overlapping labels:
```{r}
# filter data
data_filtered <- gaplong0 %>%
  filter(year > 1990) %>%
  filter(continent == c("Africa", "Asia"))

# Plot scatterplot
library(ggrepel)
data_filtered %>%
  group_by(country) %>%
  summarise(
    lifeExp = mean(lifeExp),
    gdpPercap = mean(gdpPercap)) %>%
  mutate(
    label = case_when( # Create labels to label extreme values
      lifeExp <= 40 ~ country,
      lifeExp >= 70 ~ country,
      TRUE          ~ '')) %>%

  ggplot(aes(x = lifeExp,
             y = gdpPercap,
             colour = country,
             label = label)) +
  geom_point(stat = "summary", fun = "mean") +
  geom_text_repel(box.padding = 1) + # Add text labels
  theme_minimal() +
  theme(legend.position = "none")
```

Continuous year scale
```{r}
ggplot(data=valuesbyyear[valuesbyyear$continent %in% c("Asia", "Africa") & valuesbyyear$year > 1990,],
       aes(x=gdpPercap, y=lifeExp, colour=year, shape=continent)) + geom_point()
```

Good but legend requires lots of space
```{r}
gaplong1$year<-as.numeric(gaplong1$year)
test2<-gaplong1%>%
  filter(year>1990,continent=="Africa" | continent=="Asia")

pl1<-ggplot(test2, aes(x=lifeExp,y=gdpPercap))+geom_point(aes(color=country))+theme(legend.text=element_text(size=7))

plot(pl1)
```

More simple theme
```{r}
ggplot(
  data = df_long[
    df_long$continent %in% c("Africa", "Asia") & df_long$year > 1990, ],
  mapping = aes(x = lifeExp, y = gdpPercap)) +
  geom_point() +
  theme_bw()

```


4. Split the data set by years into 12 datasets stored in a list

Base-R:
```{r}
spliteddata <- split(x = gaplong1, f = gaplong1$year)
```

```{r}
yeartables <- list()
for (year in years) {
  yeartables[[as.character(year)]] <- valuesbyyear[valuesbyyear$year == year,]
}

```


Tidyverse:
```{r}
years <- as.integer(levels(as.factor(gaplong1$year))) # vector of years

# Split data
data_years_list <- gaplong1 %>%
  group_by(year, .add = TRUE) %>%
  group_split()
```


5. Write some code that takes each of the newly formed X datasets and output the number of rows, and mean and variance of pop, and write them into a separate file.

A tidyverse-based function:
```{r, eval=FALSE}
myfun <- function(x)
{
  assertthat::assert_that(is.list(x))
  res <- x[[1]] %>% summarise(n=n(), mean= mean(pop), var=var(pop))
  write.csv(res, file = paste0("Data/", names(x), ".csv"))
}

for (i in 1:length(spliteddata))
{
  myfun(spliteddata[i])
}
```

Tidyverse pipe:
```{r, eval=FALSE}
data_years_list_metadata <- data_years_list %>%
  map_dfr( ~ .x %>% summarise(rows = nrow(.x),
                              meanPop = mean(pop),
                              variancePop = var(pop),
                              year = years))
saveRDS(data_years_list_metadata, file = "years_metadata.rds")
```

Base R loops
```{r, eval=FALSE}
for (year in years) {
  yeartable <- yeartables[[as.character(year)]]
  print(paste("Year:", as.character(year), sep=" "))
  print(paste("Rows:", nrow(yeartable), sep=" "))
  print(paste("Mean population:", mean(yeartable$pop), sep=" "))
  print(paste("Population variance:", var(yeartable$pop), sep=" "))
  write.csv(yeartable, file = paste("gapminder_", year, ".csv", sep=""))
}
```

```{r, eval=FALSE}
df2<-cbind("year","numb_rows","mean_pop","var_pop")

for (i in 1:length(test3)){
  tx<-test3[i]
  txx<-as.data.frame(tx)
  names(txx)<-c("continent","country","year","gdpPercap","lifeExp","pop")
  yr<-unique(txx$year)
  num_rows<-length(txx$year)
  mean_pop<-mean(txx$pop)
  var_pop<-var(txx$pop)
  df1<-cbind(yr,num_rows,mean_pop,var_pop)
  df2<-rbind(df2,df1)
}

df_1<-as.data.frame(df2)
df_1<-df_1[-1,]
df_1
write.csv(df_1,"results/mean_var_pop.csv")
```

do.call base-R version:
```{r, eval=FALSE}
summary_df <- do.call(rbind,
  lapply(df_list, function(a){
    data.frame(
      n_rows = nrow(a),
      mean_pop = mean(a$pop, na.rm = TRUE),
      var_pop = var(a$pop, na.rm = TRUE)
    )
  }))

# optionally save
write.csv(summary_df, "summary_results.csv", row.names = FALSE)
```

# Sources and further reading

Advanced R by Hadley Wickham https://adv-r.hadley.nz/index.html (contains much of what we talked about and more advanced topics)

http://swcarpentry.github.io/r-novice-gapminder/ (very introductory but go through it if you feel like you miss basic knowledge on a specific topic)


<!-- # A bit of data-table -->


<!-- ## Data.table + tidyverse -->

<!-- It is possible to achieve speed performance close to those of data.table with tidyverse syntax -->

<!-- ```{r} -->
<!-- library(tidyverse) -->
<!-- library("dtplyr") -->
<!-- ``` -->

<!-- # Data-table -->

<!-- The `data.table` package is popular because it provides a faster alternative to data frames when dealing with large amounts of data. -->

<!-- ```{r} -->
<!-- library(data.table) -->
<!-- data("mtcars") -->
<!-- mtcars$carname <- rownames(mtcars) -->
<!-- setDT(mtcars) -->

<!-- mtcars[mpg>20, 1:11, carb] -->
<!-- mtcars[mpg>20, 1:11] -->
<!-- mtcars[mpg>20, 1:10] -->


<!-- myvar <- "mpg" -->
<!-- mtcars[,myvar, with=F] -->
<!-- mtcars[, ..myvar] -->

<!-- mtcars[, 1 , with=F] -->
<!-- mtcars[, 1] -->

<!-- DT <- data.table(A=1:5) -->
<!-- DT[ , X := shift(A, 1, type="lag")] -->
<!-- DT[ , Y := shift(A, 1, type="lead")] -->
<!-- shift() -->

<!-- mtcars[, cyl_gear2 := cyl + gear] -->

<!-- myvar <- c('var1') -->
<!-- mtcars[, (myvar):=1] -->

<!-- mtcars[, c("myvar", "..myvar", "var1") := NULL] -->

<!-- mtcars[, mileage_type := ifelse(mpg>20, "high", "low")] -->

<!-- mtcars[, .(mean_mileage=mean(mpg)), by=cyl] -->
<!-- mtcars[, mean_mileage:=mean(mpg), by=cyl] -->

<!-- mtcars[, .I] -->
<!-- mtcars[, .N] -->

<!-- mtcars[, .I[cyl==6]] -->
<!-- mtcars[, which(cyl==6)] -->

<!-- #Compute the number of cars and the mean mileage for each gear type. -->

<!-- mtcars[,c(.N, .(mean_mileage=round(mean(mpg),2))), by=gear] -->

<!-- mtcars[, .(.N, mileage=mean(mpg) %>% round(2)), by=gear] -->

<!-- mtcars[, .(mean_mpg=mean(mpg), -->
<!--                      mean_disp=mean(disp), -->
<!--                      mean_wt=mean(wt), -->
<!--                      mean_qsec=mean(qsec)), by=cyl][order(mean_mpg), ] -->

<!-- mtcars[, .SD, by=cyl] -->

<!-- mtcars[, lapply(.SD[, 1:10, with=F], mean), by=cyl] -->

<!-- mtcars[, lapply(.SD[, -12, with=F], mean), by=cyl] -->

<!-- setkey(mtcars, carname) -->

<!-- key(mtcars) -->


<!-- dt1 <- mtcars[,.(carname, mpg, cyl)] -->
<!-- dt2 <- mtcars[1:10, .(carname, gear)] -->

<!-- dt1[dt2] -->

<!-- dcast.data.table(mtcars, cyl ~ carb, fun.aggregate = list(min, max), value.var = 'mpg') -->
<!-- dcast.data.table(mtcars, carb ~cyl , fun.aggregate =function(x) max(x)-min(x), value.var = 'mpg') -->


<!-- m = matrix(1,nrow=100000,ncol=100) -->
<!-- DF = as.data.frame(m) -->
<!-- DT = as.data.table(m)     -->

<!-- rnorm(n = ) -->

<!-- system.time(for (i in 1:10000) DF[i,1] <- i) -->
<!-- #> 1.11 seconds -->
<!-- system.time(for (i in 1:10000) DT[i,V1:=i]) -->
<!-- #> 2.21 seconds  ( -->
<!-- system.time(for (i in 1:10000) set(DT,i,1L,i)) -->
<!-- #> 0.018 seconds -->

<!-- system.time(for (i in 1:10000) set(DF,i,1L,i)) -->

<!-- system.time(DF %>% mutate(V1=1:100000)) -->

<!-- system.time(DF[,1L] <- 1:100000) -->
<!-- system.time(DT[,1L] <- 1:100000) -->


<!-- xx <- DF %>% select(V1) %>% mutate(V1=rnorm(100000)) %>% tibble() -->
<!-- xx -->

<!-- system.time(DF %>% select(V1) %>% mutate(V1=rnorm(100000)) %>% tibble()) -->


<!-- system.time(DF %>% select(V1) %>% mutate(V1=rnorm(100000), -->
<!--                                          V2=rnorm(100000), -->
<!--                                          V3=rnorm(100000), -->
<!--                                          V4=rnorm(100000), -->
<!--                                          V5=rnorm(100000), -->
<!--                                          V6=rnorm(100000), -->
<!--                                          V7=rnorm(100000), -->
<!--                                          V8=rnorm(100000))) -->


<!-- Dtib <- tibble(DF) -->
<!-- system.time(Dtib %>% select(V1) %>% mutate(V1=rnorm(100000), -->
<!--                                          V2=rnorm(100000), -->
<!--                                          V3=rnorm(100000), -->
<!--                                          V4=rnorm(100000), -->
<!--                                          V5=rnorm(100000), -->
<!--                                          V6=rnorm(100000), -->
<!--                                          V7=rnorm(100000), -->
<!--                                          V8=rnorm(100000))) -->

<!-- dt <- lazy_dt(DF) -->

<!-- system.time(dt %>% select(V1) %>% mutate(V1=rnorm(100000), -->
<!--                                          V2=rnorm(100000), -->
<!--                                          V3=rnorm(100000), -->
<!--                                          V4=rnorm(100000), -->
<!--                                          V5=rnorm(100000), -->
<!--                                          V6=rnorm(100000), -->
<!--                                          V7=rnorm(100000), -->
<!--                                          V8=rnorm(100000))) -->

<!-- ``` -->





<!-- ```{r} -->
<!-- box_extents <- expand.grid( -->
<!--   x_min = seq(110, 154, 1), -->
<!--   y_min = seq(-45, -9, 1)) -->
<!-- box_extents$x_max <- box_extents$x_min + 1 -->
<!-- box_extents$y_max <- box_extents$y_min + 1 -->

<!-- # extract CAPAD within each degree square -->
<!-- extent_list <- split(box_extents, seq_len(nrow(box_extents))) -->
<!-- ``` -->

